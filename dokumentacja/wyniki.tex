\section{Pytania bez odpowiedzi}
Ze względu na fakt, że mój miniprojekt ma charakter czysto akademicki, to uzyskane wyniki oceniam jako bardzo ciekawe i zadowalające. Mam jednak świadomość (a może tylko mi się wydaje że mam \dots), że zaprezentowana przeze mnie metoda rozwiązania problemu transkrypcji języka naturalnego na kod wykonywalny nie jest doskonała, a wiele pojawiających się zagadnień wymaga gruntownego przestudiowania. Poniżej omówię kilka zagadnień problemowych, które na tę chwilę pozostają bez rozwiązania. 
\subsection{Ocena jakości danych uczących}
Pierwsza rzecz, która sprawia mi trudność to ocena jakości dostarczonej przeze mnie próbki uczącej. Jak ocenić, czy dane nie są zbyt schematyczne, a wytrenowana sieć zachowuje swoje zdolności generalizacyjne. Może dobór przykładów, lub lepsza ich konstrukcja wpłynęłaby na poprawę osiągniętych wyników. Nie znam sposobu na zinterpretowanie dosyć lakonicznego raportu z procesu uczenia \ref{raport_trening}. Zastanowienie budzi również niska wartość prawdopodobieństwa rozpoznawalności poszczególnych encji. Dlaczego bardzo rzadko jest ona większa niż 0.2 ?
\subsection{Rozszerzalność próbki danych uczących }
Kolejna rzecz, która wymaga poprawy to sposób rozszerzania zakresu rozpoznawalnych przez algorytmy schematów. Co należałoby zrobić by sieć zaczęła rozpoznawać inny typ warunku np. frazy typu ,,czy pole xxx zostało zdefiniowane'', albo jak sprawić by sieć potrafiła rozpoznać n - grup warunkowych połączonych operatorem logicznym (w tej chwili radzi sobie z dwoma warunkami połączonymi jednym operatorem). Czy dostarczenie kolejnej liczby przykładów obejmujących taki rodzaj warunku na pewno rozwiąże problem ? Jeśli tak to jak ocenić optymalną dla próbki liczbę nowodostarczonych przykładów. Czy liczba przykładów w nowym schemacie nie powinna zachować jakiejś proporcji względem liczby próbek w starym schemacie ? Tu obawiałbym się sytuacji, że zbyt duża liczba próbek w nowym schemacie mógłaby spowodować że stanie się on dominujący, co z kolei doprowadziłoby do pogorszenia dotychczasowych osiągnięć. Pozatym takie dołączanie kolejnych przypadków szybko doprowadzi do zbyt dużego rozdrobnienia przykładów i lawinowy rozrost liczności próbki.
\subsection{Optymalny dobór parametrów uczenia}
Sam proces treningu posiada możliwości parametryzacji. Ja skorzystałem z tych najbardziej podstawowych. Może bardziej świadomy ich dobór doprowadziłby do otrzymania lepszych wyników.
\subsection{Odporność na błędy i zaburzenia}
Tutaj największym problemem jest utrzymanie zgodności z założonym schematem, oraz ocena czy proces rozpoznania przebiegł poprawnie. Nie znam na to innego sposobu niż ocena ekspercka, oczyma człowieka. Jak ustrzec się przed tym, że osoba wpisująca regułę nie wzbogaci jej treści we frazy, które są całkowicie nierozpoznawalne. Proces rozpoznawania encji i tak się zakończy powodzeniem, ale tokeny mogą zostać rozpoznane nieprawidłowo. Jak zdiagnozować taką sytuację ?
\subsection{Optymalny dobór narzędzi i technologii}
W tym punkcie należałoby się zastanowić, czy wybór biblioteki \textit{OpenNLP, techniki NER}, oraz algorytmu jest optymalny do rozwiązania tego typu zadań. Co prawda w świecie Java możliwości wyboru bibliotek są bardziej ograniczone niż w środowisku \textit{Python}, to jednak należałoby zrobić pod tym kątem rozpoznanie chociażby narzędzi \textit{Stanford NLP}.